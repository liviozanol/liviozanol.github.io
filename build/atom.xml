<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://livio.zanol.com.br/</id>
    <title>Livio's Dump Blog</title>
    <updated>2022-04-07T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://livio.zanol.com.br/"/>
    <subtitle>Livio's Dump Blog</subtitle>
    <icon>https://livio.zanol.com.br/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Using vSphere Distributed Switch (VDS) for stateless "traffic filtering" (or ACL)]]></title>
        <id>vmware-vds-traffic-filter</id>
        <link href="https://livio.zanol.com.br/vmware-vds-traffic-filter"/>
        <updated>2022-04-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Intro]]></summary>
        <content type="html"><![CDATA[<h2>Intro</h2><p>VMWare has a nice product called <a href="https://www.vmware.com/br/products/nsx.html">NSX-T</a> which allows you to build a sort of a cloud provider network infrastructure for your virtualized environment  (and K8s). It has network segmentation, routing, NAT, stateful firewall, IDS, traffic inspection, etc.</p><p>What if you already have a good and working cloud-like architecture and just want to insert some simple ACLs directly on the virtualization layer to outsource traffic filtering directly to the hypervisor, and do not want fancy things like firewall, NAT, IDS, etc. ?</p><p>vSwitch doesn&#x27;t support this; NSX-T does, but it is expensive... What about VDS???</p><p>Python scripts used available at git repository: <a href="https://github.com/liviozanol/vmware_vds_traffic_filte">https://github.com/liviozanol/vmware_vds_traffic_filte</a></p><p>Well, using vCenter web UI (you can use this <a href="https://www.vmwarelearningplatform.com/HOL/console/lab/HOL-2211-01-SDC-HOL">Hands-on Lab</a> to check), if you navigate to the VDS itself, and select a portgroup and click configure, you will see that it has some sort of Traffic Filtering. So, despite this almost hidden function (poor UI design in my opinion) it CAN filter traffic.</p><p><img src="./img/portgroup.png" alt="Port Group on vSphere"/></p><p>If you snoop the web interface a little bit you will  see that this looks like a normal network equipment ACL using the classic 5 tuples (src IP, dst IP, L4 protocol, src port, dst port) and input/output direction to apply the filter.</p><p><img src="./img/create_rule.png" alt="Creating rules on web UI"/></p><p>Ok... So, it may be possible to apply filters, but this UI is not helpful at all to create and manage a bunch of ACLs (nor to give end-users permission to apply it). Lets try with the API.</p><h2>vSphere API</h2><p>vCenter has a &quot;new&quot; REST API, launched since version 6.5. But if you look at their <a href="https://developer.vmware.com/apis/vsphere-automation/latest/vcenter">documentation</a> you will see that it is <strong>very</strong> limited and still needs to be involved to be useful for automation. Hey VMWARE, come on, prioritize vSphere REST API on your product backlog... Oh wait! Maybe you don&#x27;t want to for some reason?</p><p>:::note
<em>As a side note, NSX-T has a pretty complete, good structured and <a href="https://developer.vmware.com/apis/976">well documented REST API</a></em>
:::</p><p>But vSphere also has another &quot;Web Service API&quot; that can be accessed <a href="https://developer.vmware.com/apis/1192/vsphere">here</a>. It is very complete and you will be able to do almost anything with it, and in fact, it is used by ansible VMWare modules. The documentation is not as good as NSX-T and it is not REST, but it is something.</p><p>If you search for &quot;DvsTraffic&quot; on Data Object Types, you will find some interesting things. Let&#x27;s check <a href="https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.dvs.DistributedVirtualPort.TrafficFilterConfig.html"><code>DVSTrafficFilterConfig</code></a>: </p><ul><li>You will see that it supports one property called <code>DvsTrafficRuleset</code>;</li><li>Digging in <a href="https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.dvs.TrafficRuleset.html">DvsTrafficRuleset</a>, it supports <code>enabled</code>, some other properties and <code>DvsTrafficRule</code> object. Let&#x27;s go deeper;</li><li><a href="https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.dvs.TrafficRule.html">DvsTrafficRule</a> supports <code>action</code> which could be used to permit or deny traffic on match (besides other actions). Also supports <code>sequence</code>, <code>direction</code> and <code>qualifier</code> fields. As stated on the page about qualifier:
    - List of Network rule qualifiers. &#x27;AND&#x27; of this array of network rule qualifiers is applied as one network traffic rule. If the TrafficRule belongs to DvsFilterPolicy : There can be a maximum of 1 <code>DvsIpNetworkRuleQualifier</code>, 1 DvsMacNetworkRuleQualifier and 1 DvsSystemTrafficNetworkRuleQualifier for a total of 3 qualifier.
    - Let&#x27;s check <code>DvsIpNetworkRuleQualifier</code> than...</li><li>Hey! <a href="https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.dvs.TrafficRule.IpQualifier.html">DvsIpNetworkRuleQualifier</a> supports the 5 tuples!
    - Look! It supports <code>tcpFlags</code> also! Maybe we could simulate an &#x27;established&#x27; ACL keyword, creating an ACL which matches only segments with ACK and/or RST!
    - Oh no... Look at the description... &quot;<em>TCP flags are not supported by Traffic Filtering</em>&quot;. Ok, but we can still create traffic rules...</li></ul><p> 
:::note
This TCP Flags thing really got me the first time I tried to use DVS filter because this text was only added on documentation for version 7.0... So I tried to use this on the 6.5 version without success but could never figure out the reason that it wasn&#x27;t working... <a href="https://developer.vmware.com/apis/358/vsphere/doc/vim.dvs.TrafficRule.IpQualifier.html">Check the old version without this text</a>.
:::</p><p>So, to effectively apply ACLs on DVS you need to:</p><ul><li>Define the 5 rule tuples using <code>DvsIpNetworkRuleQualifier</code>.</li><li>Append it to a <code>qualifier</code> on a <code>DvsTrafficRule</code>.</li><li>Set <code>DvsTrafficRule</code> <code>description</code>, <code>direction</code> and <code>action</code>.</li><li>Append this <code>DvsTrafficRule</code> to a <code>DvsTrafficRuleset</code>.</li><li>Set <code>DVSTrafficFilterConfig</code> with the <code>DvsTrafficRuleset</code> you just created.</li><li>Apply <code>DVSTrafficFilterConfig</code> to a port group.</li><li>Enable traffic filtering on that port group.</li></ul><p>Some questions about this VDS traffic filter rise:</p><ul><li>How many rules can I create?
    - Well, there isn&#x27;t any documentation about this limit and VMWare <strong>must</strong> provide support to your vSphere even if you apply a long list of rules. Well, at least in theory, but I doubt that if you apply 10.000+ rules on your production environment VMWare will be able to help you...</li><li>Is it stable? How is latency (ms overhead) and throughput (less pps) impacted by rules? What about the bare metal CPU usage (could increase since it&#x27;s processing traffic to match rules)?
    - Really don&#x27;t know... You could build some test scenarios to validate.</li><li>Where/When the rules are processed.
    - Don&#x27;t know... Maybe it is like the DFW rules from NSX-T and they are processed on kernel hooks giving a good performance... But haven&#x27;t looked for it.</li><li>Can I apply ACLs for an specific VM? Can I apply to uplinks?
    - YES! You can!</li></ul><p>:::danger
Be careful when using API to manipulate trafficRules like this. There are some functionalities that are only available through API! If you create some certain kind of rules using API you may be not able to see it on WEB UI! Worst: No errors are shown, but the rules are still there! If you want to delete or edit them, you&#x27;ll need to do it using the API itself.
:::</p><h2>Python Scripts</h2><p>The vSphere API described before is commonly used through an SDK previously built by vmware itself. A lot of people use PowerShell SDK (called <a href="https://developer.vmware.com/web/tool/12.5.0/vmware-powercli">PowerCLI</a>). I like the python SDK called <a href="https://github.com/vmware/pyvmomi">pyvimovi</a> which is also used by ansible modules.</p><p>I&#x27;ve uploaded 2 scripts to github. The first one lists rules on DVS and the second creates rules on DVS. Could have been only one, but that&#x27;s ok...</p><p><a href="https://github.com/liviozanol/vmware_vds_traffic_filter/blob/main/list_vds_portgroup_rules.py"><code>list_vds_portgroup_rules.py</code></a> list rules for a specific portgroup. You pass dvswitch name, portgroup name and rule_id (optional) as argument to the script and all rules for that portgroup is outputted on a JSON formatted string.</p><p><a href="https://github.com/liviozanol/vmware_vds_traffic_filter/blob/main/edit_vds_portgroup_rules.py"><code>edit_vds_portgroup_rules.py</code></a> edit, create and delete rules for a specific portgroup using a JSON file as source. The JSON file needs to have a specific format. In the following example we intended to create a simple TCP rule on port 53 to google DNS. Keep in mind that <a href="https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml">protocol number must follow IANA definition</a>: (this example is also on script help):</p><pre><code class="language-JSON">[
    {
    &quot;sourceAddress&quot;:&quot;any&quot;,
    &quot;destinationAddress&quot;:&quot;8.8.8.8/32&quot;,
    &quot;sourcePort&quot;:&quot;1024-60000&quot;,
    &quot;destinationPort&quot;:&quot;53&quot;,
    &quot;protocol&quot;:&quot;6&quot;,
    &quot;action&quot;:&quot;accept&quot;,
    &quot;direction&quot;:&quot;both&quot;,
    &quot;description&quot;:&quot;DNS google&quot;
    }
]
</code></pre><h2>Conclusion</h2><p>vSphere Distributed Switch (VDS), along with other features like QoS, mark, shaping, LACP, etc. also supports stateless traffic filtering using 5 tuples.</p><p>Since I never tested it on production, I can&#x27;t recommend you to do so. If you have some case scenarios for this or have already tested this on a production environment, please, feel free to send some data or feedback so I can share with others.</p><p>You could use these scripts to automate VDS using ansible directly (just install pyvimomi) calling it as a &quot;shell&quot; command. Or, you could use it to create an ansible module.</p><p>Any feedback is appreciated.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 11 - Conclusions]]></title>
        <id>full-stack-it-automation-part-11</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-11"/>
        <updated>2022-02-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Conclusion]]></summary>
        <content type="html"><![CDATA[<h2>Conclusion</h2><p>This series of posts demonstrated how to use some nice tools to build a simple full-stack automation solution that can be  used to automate almost everything on your infrastructure.</p><p>The architecture demonstrated is not static and can be modified to best fit a team/company needs.</p><p>You can (and should) separate the API on 2 layers: One to deal with data validation and business rules (this is accessed by the user); One with basic validation, parsing and uploading data on gitlab.</p><p>You can make an audit function, either together on the pipeline/awx or separated. You gather info from your automated element (eg.: router) and compare it with the information on gitlab (reverse parsing it) to be sure that what was sent is really on the automated element.</p><p>You can (and really should) add a bastion host to access your automated elements, and make AWX use, for example, SSH tunnel prior to connecting to the automated element, protecting it even more from undesirable access.</p><p>To build the full-stack automation from scratch, were spent the following estimate hours:</p><ul><li>Automate AWX installation and customization it using its APIs: <strong>~ 8 hours</strong>.<ul><li>Some bugs and CentOs deprecated version really impacted on this. Had some previous knowledge on how to use AWX API, which helped.</li></ul></li><li>Automate installation and customization of Gitlab, Gitlab CI and 2 Hashicorp Vault: <strong>~ 4 hours</strong>.<ul><li>These are pretty straightforward.</li></ul></li><li>Build the ansible playbook and test it: <strong>~ 6 hours</strong>.<ul><li>Jinja templates and language are difficult to understand. Had previous knowledge on playbooks, but very few on jinja templates.</li></ul></li><li>Build the API using <a href="https://fastapi.tiangolo.com/">Python FastAPI</a>: <strong>~ 4 hours</strong><ul><li>Had some basic knowledge of python dev and zero knowledge on Fast API. Very nice framework with easy learning curve.</li></ul></li><li>Build and test the main gitlab CI/CD script which is responsible to get modified data and push to AWX: <strong>~ 1,5 hour</strong>.<ul><li>Had some previous knowledge on gitlab-CI.</li></ul></li><li>Build the CI/CD for the APIs, Bastion Host and Web Interface: <strong>~ 2 hours</strong><ul><li>Had some previous knowledge on gitlab-CI.</li></ul></li><li>Make the web interface using basic React Admin elements: <strong>~ 4 hours</strong>.<ul><li>Had some previous knowledge on React Admin.</li><li>Adjusting the queries from/to the API took most of the time.</li><li>Making the basic list/edit page was easy (~ 2 hours?). You can see that <a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/fullstack-ui/src/wanSite.js">the code is simple</a>.</li><li>Making complex things requires good knowledge on JS/React JS.</li></ul></li></ul><p>:::note
Feel free to contact me by e-mail: livio at zanol.com.br to further discuss this subject.
:::</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 10 - Demo - UI]]></title>
        <id>full-stack-it-automation-part-10-demo-ui</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-10-demo-ui"/>
        <updated>2022-02-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Web Interface]]></summary>
        <content type="html"><![CDATA[<h2>Web Interface</h2><p>In this section we will build our web interface. Users can use it to get and update info from their WAN sites in a friendly interface. Note that using web interface is not mandatory. In fact, the web page will only convert data to/from API to show on a page, but users still request our API directly and can use it to make its changes. In some aspect this is good, since you give option to users to use any other tool to interact with our solution. They can use for example <a href="https://www.postman.com">postman</a>, <a href="https://curl.se">curl</a>, ansible, terraform, or even building its own web interface if they want to.</p><p>Will be used <a href="https://marmelab.com/react-admin">react admin</a> as framework to build our UI. Their community version is very complete and allow us to build our pages (if they are basic ones) pretty easy. The Tutorial and Manual are very complete, and if want to deep dive in it go check it out.</p><p>React admin is a framework built in <a href="https://pt-br.reactjs.org">React</a> that lets you build a complete admin dashboard for almost anything using react and Javascript/Typescript (you can also use other <a href="https://www.npmjs.com/">NPM</a> modules). It integrates with APIs pretty simple and also have a lot of pre-built fields/functions that makes the job of building dashboards much more simple. As UI components, it uses the great <a href="https://mui.com/">Material UI</a> providing a great visual experience.</p><p>It&#x27;s not part of this post to explain how Javascript/Typescript, NPM or React works as this is a whole world for itself. If want you can learn more about them somewhere else on Internet.</p><p>As always, all files generated on this section (and on previous) are on full-stack automation github repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>), so you don&#x27;t need to create it yourself again.</p><p>:::note
Please note that I&#x27;m not a developer. You shouldn&#x27;t trust my code blindly (in fact, any code). But feel free to use it, change it and send me angry feedbacks and comments.
:::</p><h2>Coding the interface</h2><p>To build our simple web interface using React Admin we need basically these things:</p><ul><li>A file with our <a href="https://marmelab.com/react-admin/Authentication.html">Auth provider</a> that will receive username/pass as input and validate user permission on the API. Will be named <code>authProvider.js</code></li><li>A file with our <a href="https://marmelab.com/react-admin/DataProviders.html">Data provider</a> that will query our API and transform the JSON response from it to a format compatible with react admin. If needed, you can also convert data from react admin format to a format understood by the API before you send to the API (we have done this). Will be named <code>dataProvider.ts</code></li><li>A file with our Wan Sites functions that will basically have 2 main exported functions: 1 with a <a href="https://marmelab.com/react-admin/List.html">List</a> of Wan Sites and 1 with the <a href="https://marmelab.com/react-admin/CreateEdit.html">Edit(Update)</a> view that user will use to edit a Wan Site configuration. We also added react admin simple form validation for each field on the Edit view so user can have a better experience, but this is not required since our API <em>must</em> validate the data also. This file will be named <code>wanSite.js</code></li><li>The <code>App.js</code> file that will put everything together and make the react admin app available to the user.</li></ul><p>On the demo we will use async site update. Once the user submits a change, after validation, data is updated on gitlab and user can make other tasks. So, we need a way to show users if their submitted changes have been implemented on equipment or not and we will create another file for this: <code>jobList.js</code>. This file is a custom React file that shows a table of the last 5 jobs submitted by users and its status. It will be used as an <a href="https://marmelab.com/react-admin/CreateEdit.html#aside-component">Aside Component</a> on the Edit View that will be shown on the right side of this page to the user.</p><p>If you follow <a href="https://marmelab.com/react-admin/Tutorial.html">react admin tutorial</a> after you create your react app, edit App.js to import authProvider, dataProvider and wanSite, obviously create these files (and jobList), make the ajustment on resources and authprovider on App.js and that&#x27;s it. You can run your app (<code>yarn start</code> or <code>npm run start</code>) to see it working. If you have access to the API from the computer where you are running the react admin interface you can start to use the app. Login with &quot;client_a_user&quot; (user/pass), &quot;client_b_user&quot;(user/pass) and &quot;admin&quot; (user/pass) to see the differences.</p><p>If you look at the source codes of the <a href="https://github.com/liviozanol/full-stack_automation/tree/master/demo/fullstack-ui/src">files</a> you will see that coding basic things using react admin is pretty simple and the documentation and examples help a lot. But as you start to customize elements, things can get very complicated. If you take a look at <code>jobList.js</code> that is pratically a custom built element using very few react admin elements, you will see how messy things can become. Either way, React Admin always tries to make things simpler to you and sure is much more easier than build your own admin dashboard from scracth.</p><p>UI demonstration after wall is installed and configured
<img src="./img/UI.gif" alt="UI demonstration after wall is installed and configured"/></p><h2>Publishing our webpage</h2><p>Just like our API, everything will be auto deployed by our gitlab runner using a CI/CD pipeline.</p><p>We will again use the same gitlab runner that calls AWX. On a production environment, the UI and API runner should be separated.</p><p>Since we will be using React Admin for our UI, we will use <a href="https://www.npmjs.com/">NPM</a> to build and install our App. Our repo will contain a <code>packages.json</code> file that is read by npm and installs the required packages and its dependencies. The output from this build are simple HTML, CSS and JS files that you can even run on your local computer. The CI/CD pipeline will just get these files, and build a simple nginx docker image with then inside, publishing our app just like any simple simple HTML/JS application.</p><p>Will be created 1 repository for our react admin source files and packages.json. Gitlab runner will watch this repo and update our UI container everytime we modify it.</p><p>Again, our CI/CD pipeline will be VERY simple, without even testing our application before deploy. Please note that adding more stages with some good tests, validation, approval is very recommended. For simplicity, the update process will be disruptive (stop old containers and running new ones). On production environment, obviously, you should make something more suitable for your needs.</p><p>:::note
The installation/configuration script will try to guess which IP address is the pyshical IP and change App.js to use it as a dataprovider. If its wrong, you can just update App.js file with the correct IP, either using git operations or gitlab web UI.
:::</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 9 - Demo - API]]></title>
        <id>full-stack-it-automation-part-9-demo-api</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-9-demo-api"/>
        <updated>2022-01-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Service API]]></summary>
        <content type="html"><![CDATA[<h2>Service API</h2><p>In this section we will build our service API. It&#x27;s responsible to <em>receive direct user request</em>, check permissions and if authorized, read and modify files on our gitlab repository.</p><p>We will use python as the API language, but as explained before, you could use any language you want that can be built to receive and process HTTP requests. To make things a little faster, will be used <a href="https://fastapi.tiangolo.com/">FastAPI framework</a>. We will try to keep things as simple as possible for the demo, and some funtions may have some minor issues, but on your journey to build a production API you should take more cautions and make things as secure as possible.</p><p>After building the API, users should be able to access it using user/pass and view or change information from their WAN site.</p><p>All files generated on this section (and on previous) are on full-stack automation github repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>), so you don&#x27;t need to create it yourself again.</p><p>:::warning
Please, note that in this demo we will use Hashicorp Vault 2 itself to store and verify users authentication and authorization with simple Key/Value and this is not safe/recommended at all. We will create 3 KV secrets in it, each one containing the username, the password and the tenants that each user can access. API simply queries this data and check if passwords match and checks which tenant users belong. Also, queries to the API will use a Basic HTTP Authentication using provided user/pass and that is also not safe and should not be used on production.</p><p>On a production environment you <em>MUST</em> use other method of authentication/authorzation.</p><p>If you already have an auth system you should consider integrating your APIs with it. If its an <a href="https://openid.net/connect/">OIDC</a>/<a href="https://oauth.net/2/">OAuth2</a> even better, you can use its own specification to authorize and get users tenant based on custom scopes and it also has it own mean of temporary token exchange to enhace security.</p><p>If you want to build your own Auth, you need to get this information elsewhere. I like <a href="https://www.youtube.com/watch?v=d4Y2DkKbxM0">this</a> simple tutorial using React/GO that even uses JWT to exchange messages to/from API. You could also use some free OIDC as <a href="https://www.ory.sh/hydra/">Ory</a> to build on premise auth system, and we could have done this on this demo, but would make our demo too big, more than it already is. Python FastAPI have nice features to easily integrate with oAuth/OIDC.
:::</p><p>:::note
Please note that I&#x27;m not a developer. You shouldn&#x27;t trust my code blindly (in fact, any code). But feel free to use it, change it and send me angry feedbacks and comments.
:::</p><p>API source code for our demo can be found <a href="https://github.com/liviozanol/full-stack_automation/tree/master/demo/api">here</a></p><p>Bastion configuration for our demo can be found <a href="https://github.com/liviozanol/full-stack_automation/tree/master/demo/bastion">here</a></p><h2>API structure</h2><p>When you build APIs you almost always want to think your serevice on a <a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a> way:</p><ul><li>How do you Create a service instance;</li><li>How do you Read/List your serivce instances;</li><li>How do you Update a service instance;</li><li>How do you Delete a service instance.</li></ul><p>Remebering our <a href="./full-stack-it-automation-part-6-demo-scenario">service definition</a>, our service is a WAN site LAN customization that users can change LAN IP addresses, set helper address, ACLs and some other infos. Thinking in a CRUD way would be something like this:</p><ul><li>Create/Enable a WAN site LAN interface (or install/activate/comission whole WAN site CPE, including its PE interface).</li><li>Read/List WAN sites LAN configuration.</li><li>Update/Change WAN sites LAN configuration.</li><li>Delete/Disable a WAN site LAN interface (or desinstall/descomission whole WAN site CPE).</li></ul><p>As you can see, Creating/Deleting our service could/should be a staff/operator activity, and we will not cover it on the demo, but if you want, you sure can! You would need to create playbooks involved on these tasks, change your gitlab runner to indetify when to execute each playbook (eg.: by a commit message?), create a way to authenticate/authorize your operators (could even use our Vault user/pass/tenant sctructure - just for a demo, please) and create these functions on the API/UI.</p><p>Considering the info above, we will build a <a href="https://restfulapi.net/">REST API</a> that receives either a HTTP GET(read/list) or PUT(update) request. The API will be stateless and you can scale it horizontally.</p><p>We will also use a bastion container using <a href="https://www.nginx.com/">NGINX</a> that receives our requests and identify to which API endpoint it should redirect it based on path (eg.: <a href="http://127.0.0.1/wan_site">http://127.0.0.1/wan_site</a> should go to our demo API). Keep in mind that each API container is responsible for its own auth function, our bastion hosts only pass our requests to the API based on the URL path (eg.: wan_site to wan_site APIs).</p><p>You could also add some cache layer if you want using something like <a href="https://redis.io/">redis</a> if you have some complex automation. </p><p>Everything will be auto deployed by our gitlab runner using a CI/CD pipeline.</p><p>:::note
You could, and it is a good practice, create 2 separated APIs layers for any specific service deployed. The first one deals with data validation, business rules and other things related with user relationship and frontend. This is the API accessed by users and the web interface. Another one would be responsible with basic validation, parsing, uploading data on gitlab and other things related with automation itself. The first one only talks to the second one and never touches any piece of your automation infrastructure. You could also add an auth for this API to API conversation using JWT for example to secure it better.
:::</p><p>The following picture demonstrate our expanded API layer for the demo (we will build a bastion host container and only one wan_site API container, but could be easilly scalable):
<img src="./img/api_arch.svg" alt="API layer architecture"/></p><p>Demo with API running
<img src="./img/api.gif" alt="API"/></p><h2>Bastion NGINX configuration</h2><p>We will keep bastion host configuration as simple as possible, and you should enhance its configuration if needed and can read more about NGINX configuration <a href="https://docs.nginx.com/nginx/admin-guide">here</a>.</p><p>We will use <a href="https://hub.docker.com/_/nginx">official alpine nginx docker image</a> and create 3 files:</p><ul><li><code>apis.conf</code> that will have our proxy_pass redirecting requests containing wan_sites on path to our API. For every API you create, just add a line like this, redirecting to the correct API endpoint. 127.0.0.1 will be replaced by our script with the real IP from our host.</li></ul><pre><code class="language-shell">location /wan_sites {
    proxy_pass http://127.0.0.1:10042/wan_sites;
    include /etc/nginx/conf.d/APIs/default_proxy.conf;
}
</code></pre><ul><li><code>default_proxy.conf</code> that will have our common proxy configuration for every API:</li></ul><pre><code class="language-shell">proxy_http_version  1.1;
proxy_cache_bypass  $http_upgrade;

proxy_set_header Host               $host;
proxy_set_header X-Real-IP          $remote_addr;
proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;
proxy_set_header X-Forwarded-Proto  $scheme;
proxy_set_header X-Forwarded-Host   $host;
proxy_set_header X-Forwarded-Port   $server_port;
    
proxy_connect_timeout 180;
proxy_send_timeout 180;
proxy_read_timeout 180;
</code></pre><ul><li><code>default.conf</code> that is basically own nginx default.conf just adding include clause to import our <code>apis.conf</code>:</li></ul><pre><code class="language-shell">server {
    listen       80;
    listen  [::]:80;
    server_name  localhost;
    include /etc/nginx/conf.d/APIs/*.conf;
(...)
}
</code></pre><h2>API code and server</h2><p>As stated on the beginning we will use <a href="https://fastapi.tiangolo.com/">FastAPI framework</a> to build our API. We will publish it using <a href="https://www.uvicorn.org/">Uvicorn</a> on a container.</p><p>You could scale it simply creating more containers and changing nginx bastion host config (if not on kubernetes) to load balance between them (please use <a href="https://docs.nginx.com/nginx/admin-guide/load-balancer/http-health-check">healthchecks</a>), or use some more structured approaches using <a href="https://fastapi.tiangolo.com/deployment/server-workers">gunicorn</a> or even more complex structures with cache layers.</p><p>The created code is simple and structured in a approach more close to Functional Programming rather than Object Oriented (OOP). It has 3 files:</p><ul><li><a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/api/main.py">main.py</a></li><li><a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/api/check_permissions.py">check_permissions.py</a></li><li><a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/api/wan_api_functions.py">wan_api_functions.py</a></li></ul><p>main.py is responsible to handle the HTTP requests, check if user have permissions, and send the requests to the appropriate functions that processes them. If a user send a GET request to the path &quot;/wan_sites&quot; main.py, after checking permissions, will send the request to the function that get sites JSON data from gitlab and return them.</p><p>check_permissions.py is responsible to get credentials from the HTTP Authorization header, check if they match the stored one on vault and get the tenants that user has permission. For EVERY request the Auth Header <em>must</em> be sent to the API and requests are only processed if they are authorized on vault. Separating the permission check in functions like this, allow us to easially change how this check is done to use other auth providers like OIDC. The function can do whatever you want, just needing to return what tenants does that user has permission.</p><p>wan_api_functions.py is our principal file and is the one that we work the most coding. It contains all functions related to the API objective itself. It has functions to get data from gitlab, to list pipeline jobs and its status, to receive UPDATE requests from user and validate it and so on. The biggest and most workly function here is the one that validate the user data on a UPDATE requests. It has a lot of strictly validating rules based on our data definition and is the function that you need to have most care and takes more time to build.</p><p>:::note
We have created a new HTTP GET path on our API that will list the last x pipeline jobs and its status. This can be used by the user to check if its UPDATE is finished or not and will also be used on the web UI.</p><p>On the UPDATE API path we have configured the function to accept a query string that will make the UPDATE syncronous (wait until the job pipeline is finished) or asyncronous (just UPDATE gitlab file with user data).</p><p>We have made a separate validation function for user data because we wanted to make some complex data validation besides simple string/int/boolean/regex one. FastAPI has some good options for simple data validation using strong integration with <a href="https://pydantic-docs.helpmanual.io/">pydantic</a>
:::</p><h2>Deploying Bastion host and API using gitlab runner</h2><p>As stated before, we will use the same gitlab runner that call AWX to automatic deploy our API and bastion/reverse proxy. On a production environment, the API/bastion gitlab runner should be separated.</p><p>Will be created 2 repositories on gitlab: 1 for our bastion host and another for our API. Gitlab runner will watch these repos and update our API or bastion container everytime we modify it.</p><p>Again, <a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/api/.gitlab-ci.yml">our CI/CD pipeline</a> will be VERY simple, without even testing our application before deploy. Please note that adding more stages with some good tests, validation, approval is very recommended for the APIs and also for UI. For simplicity, also the bastion/API update process will be disruptive (stop old caontainers and running new ones), but you can easilly achieve high availability for the APIs scaling them horizontally and Load Balancing with healthchecks on bastion hosts. Updating the bastion/reverse proxy container  without disrupting it thougha is more difficult.</p><h2>Testing the API witch CURL</h2><p>When the API is deployed you can test its functions using curl like below:</p><ul><li><p>Listing Client A WAN sites
<code>curl --user client_a_user:client_a_user 127.0.0.1:10042/wan_sites | jq</code></p></li><li><p>Getting one specific site from Client B:
<code>curl --user client_b_user:client_b_user 127.0.0.1:10042/wan_sites/site_3 | jq</code></p></li><li><p>Updating info for site 1 from Client A asyncronous (please, create CHANGED_FILE.json with your changes):
<code>curl --user client_a_user:client_a_user 127.0.0.1:10042/wan_sites/site_1?sync=0 -X PUT --data @demo/api/update_test_files/CHANGED_FILE.json</code></p></li><li><p>List the last 10 jobs for site 1:
<code>curl --user client_a_user:client_a_user 127.0.0.1:10042/wan_sites/site_1/jobs?number_of_jobs=10 | jq</code></p></li></ul>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 8 - Demo - Runner]]></title>
        <id>full-stack-it-automation-part-8-demo-gitlab-runner</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-8-demo-gitlab-runner"/>
        <updated>2022-01-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Gtilab-CI/Gitlab Runner]]></summary>
        <content type="html"><![CDATA[<h2>Gtilab-CI/Gitlab Runner</h2><p>In the previous post we have defined our structured data, stored it on gitlab, created our playbooks and inventories and imported them on AWX. Now we will prepare our CI/CD pipeline to monitor changes on our data on gitlab and tell AWX to execute our playbook. Gitlab runner will be responsible for these tasks, registering on our gitlab projects/group and executing a series of tasks once our data is changed, calling AWX API to run our workflow playbook passing our modified data and monitoring our job in execution also using AWX API.</p><p>All files generated on this section (and on previous) are on full-stack automation github repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>), so you don&#x27;t need to create it yourself again.</p><p>Everytime our gitlab repository is changed, gitlab-ci will get the contents of our wan_site_cfg.json file and send it to AWX as a variable, calling a job/playbook/template or a workflow that will implement the changes on our equipments.</p><p>After concluding this section you will have something like a GitOps for our wan automation service. If you change some information from your WAN site on gitlab repo, AWX will implement it on equipment.</p><p>:::note
Please, note that in this demo we installed gitlab runner using &quot;privileged&quot; mode and also mapped docker host socket to the container. This means that gitlab runner container can virtually execute any docker command directly into our docker host, stopping, starting, running and removing ANY container (including AWX and itself). This is NOT a good practice, but we have made this way because we will use gitlab runner to also start our APIs and UI container on the demo.</p><p>In a production environment, as stated before, its a good practice to separate functions on different hosts (user APIs, UI and its gitlab runner CI/CD on a isolated place from AWX playbooks and its own gitlab runner CI/CD) and if possible creating/using more than one gitlab. The gitlab runner that starts our AWX jobs, does not need privileged access and only needs to connect to AWX and gitlab APIs, so you don&#x27;t need to map docker host socket neither use privileged mode on this runner and could use <a href="https://github.com/jpetazzo/dind">docker in docker</a> to run your curl/bash commands or execute commands directly on gitlab runner container.
:::</p><p>Every file for Gitlab Runner config to our demo can be found <a href="https://github.com/liviozanol/full-stack_automation/tree/master/demo/gitlab-ci">here</a></p><h2>Registering our runner</h2><p>To register our gitlab-CI/runner we first need a registration token and we will get this token using gitlab API. After getting this token we will <code>docker exec</code> a command on gitlab runner container to register it on the wan_site group. After runner is registered, everytime a file is changed on any repository inside wan_site group, gitlab runner will look for a .gitlab-ci.yml file and use it to start our CI/CD pipeline.</p><h2>Building our .gitlab-ci.yml</h2><p>Will be used a quite simple pipeline with only 1 stage that gets required credentials from our Vault 2 and calls AWX API to start our workflow job template and monitors it until it stops. You could insert other stages to improve your pipeline and you should consider this if you are making more complex automations. Some examples for network automation:</p><ul><li>Add a <a href="https://vincent.bernat.ch/en/blog/2021-network-jerikan-ansible">Jerikan</a> stage to to parse your data and send it already parsed to AWX (needs to change the playbook)</li><li>Add a <a href="https://www.batfish.org/">batfish</a> stage to validate your config after parsing it.</li><li>Add a <a href="https://netsim-tools.readthedocs.io/">network simulation tool</a> stage to test your parsed configs in a lab.</li><li>Add a &quot;homologation&quot; test stage, running another playbook (or changing hostname or sending to another specific AWX) that will implement your changes on some lab equipment and validate its functionalities. Could either be physical equipments or lab routers running on a qemu emulation (see <a href="https://www.eve-ng.net/">eve-ng</a> for a nice solution)</li><li>Add an approval stage making the pipeline wait until an operator access gitlab and approves the changes (or build an API/UI for this step!).</li></ul><p>Our 1 stage pipeline will use a basic alpine image from docker hub and we will install curl and jq on it to use on our APIs calls. You could (and I think it&#x27;s a good practice) build your own image only with basic Shell functionalities, curl, jq and other tools that you use on each stage.</p><p>To query Vault and get AWX user/pass we need a token and we&#x27;ll store it using the gitlab variable as &#x27;masked&#x27;. It&#x27;s the only presumably sensible information that will be stored on gitlab. Any other sensitive data that will be needed must be stored on Vault and queried from it on run time. You should restrict the access to this access token and for AWX user/pass on hashicorp Vault to only allow queries from gitlab ip address, but we won&#x27;t do this on the demo. If you are on a production environment I hope you built Vault and installed it already on an isolated environment with restricted L3/L4 access and will also make the application restriction mentioned. </p><p>We will also use 2 more variables on gitlab: Vault URL and AWX IP Address/Port. You could store the AWX ip/port on Vault if you want...</p><p>After getting the AWX credentials, our pipeline will call AWX API to run our playbook, getting the modified data from wan_site_data.json on the repository and simply passing it as an extra_vars to AWX.</p><p>At the end we have a simple while loop that keeps querying the AWX API to get the job status and either exit 0 (if the job is successful) or 1 (if it fails).</p><pre><code class="language-yml">image: alpine:3.15.0

before_script: #Installing curl and jq
  - which curl || (apk --no-cache add curl)
  - which jq || (apk --no-cache add jq)

run:
  script:
    - PLAYBOOK_NAME=&quot;wan_automation_workflow_template&quot;
    #Getting credentials from vault
    - &quot;CURL_RESULT=`curl -sk -H \&quot;X-Vault-Token: $VAULT_TWO_TOKEN\&quot; \&quot;$VAULT_TWO_URL/v1/secret/data/awx_secret\&quot;`&quot;
    - &quot;AWX_USER=`echo $CURL_RESULT | jq -r .data.data.awx_user`&quot;
    - &quot;AWX_PASS=`echo $CURL_RESULT | jq -r .data.data.awx_pass`&quot;
    - AWX_URL=&quot;https://$AWX_USER:$AWX_PASS@$AWX_ADDRESS_PORT&quot;
    #Querying AWX API to get workflow job template ID
    - &quot;JOB_TEMPLATE_ID=`curl -sk $AWX_URL/api/v2/workflow_job_templates/?search=$PLAYBOOK_NAME | jq .results[].id`&quot;
    #Calling AWX API to run our workflow
    - &quot;JOB_ID=`curl --header \&quot;Content-Type: application/json\&quot; -sk $AWX_URL/api/v2/workflow_job_templates/$JOB_TEMPLATE_ID/launch/ --data \&quot;{\\\&quot;extra_vars\\\&quot;:$(cat wan_site_data.json)}\&quot; | jq .id`&quot;
    #Monitoring workflow
    #COUNT is used as a protection so we don&#x27;t have ifnite loops
    - COUNT=0
    - MAX_COUNT=80
    - &gt;
      while true; do
        JOB_STATUS_TEXT=`curl -sk $AWX_URL/api/v2/workflow_jobs/$JOB_ID/ | jq -r .status`
        if [ &quot;$JOB_STATUS_TEXT&quot; == &quot;successful&quot; ]; then
          exit 0
        fi
        if [ &quot;$JOB_STATUS_TEXT&quot; == &quot;failed&quot; ]; then
          exit 1
        fi
        sleep 5
        COUNT=$(( COUNT + 1 ))
        if [ &quot;$COUNT&quot; -gt &quot;$MAX_COUNT&quot; ]; then
          echo &quot;protection against infinite loop reached. aborting&quot;
          exit 1
        fi
      done;
</code></pre>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 7 - Demo - AWX]]></title>
        <id>full-stack-it-automation-part-7-demo-awx</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-7-demo-awx"/>
        <updated>2022-01-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Creating Playbooks and complements]]></summary>
        <content type="html"><![CDATA[<h2>Creating Playbooks and complements</h2><p>In the previous post we have defined our structured data and created sample projects on gitlab. Now that our data structure is defined we can start to build our playbooks and templates. In this section we&#x27;ll build our inventory, templates, workflow and playbook.</p><p>All files generated on this section (and on previous) are on full-stack automation github repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>), so you don&#x27;t need to create it yourself again.</p><p>Ansible/AWX are only the executor of automation tasks and they are never accessed by end-users. So, we don&#x27;t need to have separation/segregation on a client basis on AWX and you don&#x27;t need to carry about this. More information about AWX and its role on this architecture can be found <a href="full-stack-it-automation-part-4-awx">here</a></p><p>Everytime our gitlab repository is changed, gitlab-ci will get the contents of our wan_site_cfg.json file and send it to AWX as a variable, calling a job/playbook/template or a workflow that will implement the changes on our equipments.</p><p>After configuring AWX, you should be able to start jobs on it to implement changes on remote WAN sites.</p><p>Every file for AWX config to our demo can be found <a href="https://github.com/liviozanol/full-stack_automation/tree/master/demo/awx">here</a></p><h2>Inventory</h2><p>On AWX, you can have different inventories for each Project. Considering this, let&#x27;s define our project being our service in a way that for each service we can have different jobs/playbooks, templates and inventories. You could also use any separation you like (per your company technical areas - good for permission control, per element types, per automation types, etc.).</p><p>If you need to use some kind of jump/bastion host to access your devices, you can also define it on this file with variables for all hosts, group of hosts or single hosts. You don&#x27;t need to (and shouldn&#x27;t) keep a password/key for the SSH access here, you can just point to a place where the key will be and get it from hashicorp vault. <a href="https://docs.ansible.com/ansible/latest/reference_appendices/faq.html#how-do-i-configure-a-jump-host-to-access-servers-that-i-have-no-direct-access-to">read more</a></p><p>Since we&#x27;ll be using a lot of YAML I think it&#x27;s nice to build the inventory also using YAML, but feel free to use INI as well looking at <a href="https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html">reference guide</a>.</p><p>Our YAML for this wan_site automation project can be like this:</p><pre><code class="language-yaml">
all:
  vars:
    dumb_var: you_can_have_you_global_var_here_including_ssh_args
    ansible_user: test #ansible SSH user
    ansible_ssh_common_args: &#x27;-o ControlPersist=60s -o ConnectTimeout=300 -o StrictHostKeyChecking=no&#x27; #just to test... on production, remove this
    ansible_ssh_private_key_file: ./id_rsa_fullstack.key #Key to use to login
  children:
    ROUTERS_US:
      vars:
        dumb_var2: you_can_have_you_group_var_here_including_ssh_args
      hosts:
        site_1:
          vars:
            dumb_var3: you_can_have_you_host_var_here_including_ssh_args
          ansible_host: 127.0.0.66 #device IP that accepts SSH from ansible (or from bastion/jump host)
          ansible_port: 2222 #SSH listenport
          ansible_connection: network_cli #ansible connection type
          ansible_network_os: ios #variable with device OS that can be used later on our playbook to convert a propper template file
        site_3:
          ansible_host: 127.0.0.1
          ansible_port: 10322
          ansible_connection: network_cli
          ansible_network_os: junos
    ROUTERS_ASIA:
      hosts:
        site_2:
          ansible_host: 127.0.0.1
          ansible_port: 10222
          ansible_connection: network_cli
          ansible_network_os: eos
        site_4:
          ansible_host: 127.0.0.1
          ansible_port: 10422
          ansible_connection: network_cli
          ansible_network_os: iosxr
</code></pre><p>In our example inventory files, remote sites are separated by geographical area, but you can use any separation you want. You can even have a global inventory file maintained by another team that is updated on gitlab by an ansible playbook itself (imagine an initial deploy phase).</p><p>The important thing to note here is that the &quot;hosts&quot; names are the ones used to identify on which host jobs/playbooks need to be executed. So, in our demo, <em>hosts names on inventory (e.g.: site_1) must match the project name on gitlab</em>. You could use a field on the structured data (or another file) for this and make an internal name/id that is not seen nor edited by the user, but let&#x27;s keep it simple on the demo.</p><h2>Building Playbooks and templates</h2><p>We will organize our playbooks in a way that we can run commands on equipment depending on its Operating System/type. You could do something more structured using templates, correct ansible modules for each command block (e.g.: acl, interface config, etc.) and also use roles functions separated by OS/type, but I think that a more simple approach is better for maintenance and make you less dependent on ansible and its own organization itself. A point to remember is that, since every change on infrastructure will be made through an API that has strong data constraints, data is already more validated than if we called ansible directly or modifying it via gitlab.</p><p>For each type of OS we will have 1 playbook (e.g.: ios_playbook.yml) and 1 template. They will be responsible to convert (a.k.a. parse) our structured JSON data received as variable to commands that needs to be executed on equipment. Since ACLs commands are complex to parse, we&#x27;ll be using a template to convert it. It receives our ACLs entry and converts it to CLI commands. You could also use something like <a href="https://vincent.bernat.ch/en/blog/2021-network-jerikan-ansible">Jerikan</a> on the gitlab CI and deliver your data already parsed to ansible so it only executes an action plan instead of parsing.</p><p>We will also have a &quot;parent&quot; playbook that simply point to the correct OS playbook depending on &quot;ansible_network_os&quot; variable and use &quot;import_playbook&quot; to import the correct os playbook and a dumb start.yml playbook that only register &quot;ansible_network_os&quot; variable to use on &quot;import_playbook&quot; since we can&#x27;t access ansible_network_os before import_playbook. These 2 playbooks are the one that we will create job templates on AWX. We will create a workflow template to link these two playbooks.</p><p>In this demo we&#x27;ll only construct an IOS playbook (also runs on IOS XE), but the concept is the same for any OS. Once the workflow and these 2 job templates are created on AWX, to add support to other OSes, all you need to do is to add specific OS playbooks/templates to gitlab repo.</p><p>:::note
AWX latest versions have changed a lot from some previous one. Now, playbooks are run on a AWX &quot;Execution Environment&quot; (EE) that basically is a custom container image containing commands and python modules needed. Fortunately, AWX already has a default EE with some modules. Sadly, the default EE doesn&#x27;t have netaddr python module needed to make some complex network operations more simple (eg.: creating wildcard mask from a normal mask), so we added a &quot;pip install netaddr&quot; as the first task on our playbook and delegated it to localhost.</p><p>In a production scenario you should consider building your own EE image with the netaddr module. Or maybe, when/if you build your production environment some guy on AWX EE github repo recognize this problem and accept a pull request inserting this module by default. See <a href="https://github.com/ansible/awx-ee/issues/90">this issue</a>
:::</p><p><code>ios_playbook.yml</code> example - only modifying the first interface from the JSON file submitted:</p><pre><code class="language-YAML">---
- name: Create ACLs
  hosts: &#x27;{{ site_id }}&#x27;
  gather_facts: no
  tasks:
  - name: Main Block
    block:
      - name: pip install netaddr (really disagree how red hat manage their products...). Either I do this or create a custom Execution Enviroment....
        ansible.builtin.shell: pip install netaddr
        delegate_to: 127.0.0.1

      - name: Parsing in and out ACEs and setting fact
        ansible.builtin.set_fact:
          in_aces: &quot;{{ lookup(&#x27;template&#x27;, &#x27;./ios_template.j2&#x27;, template_vars=dict(aces=lan_interfaces[0].in_acl)).splitlines() }}&quot;
          out_aces: &quot;{{ lookup(&#x27;template&#x27;, &#x27;./ios_template.j2&#x27;, template_vars=dict(aces=lan_interfaces[0].out_acl)).splitlines() }}&quot;
        delegate_to: localhost
              
      - name: Get running config interface section
        cisco.ios.ios_command:
          commands:
            - show running-config | sec interface {{ lan_interfaces[0].interface_name }}
        register: acl_name

      - name: Get ACL IN Name
        ansible.builtin.set_fact:
          in_acl: &quot;{{ acl_name.stdout_lines[0] | select(&#x27;match&#x27;, &#x27;^\\s*ip\\s+access-group\\s+(.*)in&#x27;) | map(&#x27;regex_replace&#x27;, &#x27;^\\s*ip\\s+access-group\\s+(.*)\\s+in.*$&#x27;,&#x27;\\1&#x27;) }}&quot; 
          #this regex will capture the ACL name (a string between word &#x27;access-group&#x27; and word &#x27;in&#x27;)
      - name: Get ACL OUT Name
        ansible.builtin.set_fact:
          out_acl: &quot;{{ acl_name.stdout_lines[0] | select(&#x27;match&#x27;, &#x27;^\\s*ip\\s+access-group\\s+(.*)out&#x27;) | map(&#x27;regex_replace&#x27;, &#x27;^\\s*ip\\s+access-group\\s+(.*)\\s+out.*$&#x27;,&#x27;\\1&#x27;) }}&quot; 

      #To get ansible_date_time and epoch
      - setup:
          gather_subset:
            - min
      #- debug:
      #    var: vars
      #To get ansible_date_time and epoch

      - name: Creating IN ACL
        cisco.ios.ios_config:
          lines:
            &quot;{{ in_aces }}&quot;
          parents: &quot;ip access-list extended wan_site_{{ ansible_date_time.epoch }}_in&quot;
      - name: Creating OUT ACL
        cisco.ios.ios_config:
          lines:
            &quot;{{ in_aces }}&quot;
          parents: &quot;ip access-list extended wan_site_{{ ansible_date_time.epoch }}_out&quot;
      
      - name: Attaching ACL to interface
        cisco.ios.ios_config:
          lines:
            - &quot;ip access-group wan_site_{{ ansible_date_time.epoch }}_in in&quot;
            - &quot;ip access-group wan_site_{{ ansible_date_time.epoch }}_out out&quot;
          parents: &quot;interface {{ lan_interfaces[0].interface_name }}&quot;
      
      - name: Deleting old IN ACL
        cisco.ios.ios_config:
          lines:
            - &quot;no ip access-list extended {{ in_acl[0] }}&quot;
        when: in_acl[0] is defined and in_acl[0] | length &gt; 0
      - name: Deleting old OUT ACL
        cisco.ios.ios_config:
          lines:
            - &quot;no ip access-list extended {{ out_acl[0] }}&quot;
        when: out_acl[0] is defined and out_acl[0] | length &gt; 0
    when: lan_interfaces[0].in_acl is defined and lan_interfaces[0].in_acl | length &gt; 0 and lan_interfaces[0].out_acl is defined and lan_interfaces[0].out_acl | length &gt; 0

- name: Setting other changes
  hosts: &#x27;{{ site_id }}&#x27;
  gather_facts: no
  tasks:
    - name: Parsing &quot;IP/Mask&quot; to &quot;IP Mask (dot decimal formation)&quot;
      set_fact:
        ip_mask: &quot;{{ lan_interfaces[0].ip_address | ansible.netcommon.ipaddr(&#x27;address&#x27;) }} {{ lan_interfaces[0].ip_address | ansible.netcommon.ipaddr(&#x27;netmask&#x27;) }}&quot;
      delegate_to: localhost
      when: lan_interfaces[0].ip_address is defined and lan_interfaces[0].ip_address | length &gt; 0

    - name: Changing IP address
      cisco.ios.ios_config:
        lines:
          - &quot;ip address {{ ip_mask }}&quot;
        parents: &quot;interface {{ lan_interfaces[0].interface_name }}&quot;
      when: lan_interfaces[0].ip_address is defined and lan_interfaces[0].ip_address | length &gt; 0
    
    - name: Changing Description
      cisco.ios.ios_config:
        lines:
          - &quot;description {{ lan_interfaces[0].description }}&quot;
        parents: &quot;interface {{ lan_interfaces[0].interface_name }}&quot;
      when: lan_interfaces[0].description is defined and lan_interfaces[0].description | length &gt; 0
    
    - name: Changing Helper Address
      cisco.ios.ios_config:
        lines:
          - &quot;ip helper-address {{ lan_interfaces[0].helper_address }}&quot;
        parents: &quot;interface {{ lan_interfaces[0].interface_name }}&quot;
      when: lan_interfaces[0].helper_address is defined and lan_interfaces[0].helper_address | length &gt; 0

</code></pre><p><code>ios_template.j2</code> example that will convert our ACLs JSON to IOS commands:</p><pre><code class="language-YAML">#jinja2:lstrip_blocks: True
{% for item in aces %}
    {#Check if source or dst IP address are IP/MASK or &#x27;any&#x27; and set wildcard mask#}
    {%- if item.src != &#x27;any&#x27; -%}
        {%- set src_wildcard = item.src | ansible.netcommon.ipaddr(&#x27;wildcard&#x27;) -%}
    {%- endif -%}
    {%- if item.dst != &#x27;any&#x27; -%}
        {%- set dst_wildcard = item.dst | ansible.netcommon.ipaddr(&#x27;wildcard&#x27;) -%}
    {%- endif -%}
    {##################}
    {#Make initial statement (e.g.: &quot;permit ip&quot;)#}
    {{- item.action }}{{ &#x27; &#x27; + item.protocol -}}
    {##################}
    {#Setting src address (e.g.: &quot;any&quot; or &quot;192.168.254.0 0.0.0.255&quot;)#}
    {%- if item.src == &#x27;any&#x27; -%}
        {{- &#x27; &#x27; + item.src -}}
    {%- else -%}
        {{- &#x27; &#x27; + item.src | ansible.netcommon.ipaddr(&#x27;network&#x27;) }}{{ &#x27; &#x27; + src_wildcard -}}
    {%- endif -%}
    {##################}
    {#Setting dst address (e.g.: &quot;any&quot; or &quot;192.168.254.0 0.0.0.255&quot;)#}
    {%- if item.dst == &#x27;any&#x27; -%}
        {{- &#x27; &#x27; + item.dst -}}
    {%- else -%}
        {{- &#x27; &#x27; + item.dst | ansible.netcommon.ipaddr(&#x27;network&#x27;) }}{{ &#x27; &#x27; + dst_wildcard -}}
    {%- endif -%}
    {##################}
    {#Setting dst port (e.g.: &quot;123&quot; or &quot;range 1 123&quot;). If port contains &quot;-&quot; char its a range#}
    {%- if &#x27;-&#x27; in item.port -%}
        {% set range_initial_port, range_final_port = item.port.split(&#x27;-&#x27;) %}
        {{- &#x27; range &#x27; + range_initial_port + &#x27; &#x27; + range_final_port -}}
    {%- else -%}
        {%- if item.protocol == &#x27;udp&#x27; or item.protocol == &#x27;tcp&#x27; -%}
            {{- &#x27; eq &#x27; + item.port -}}
        {%- endif -%}
    {%- endif -%}
    {{ &#x27; &#x27; }}
{% endfor %}
</code></pre><h2>Storing playbooks, templates and inventories</h2><p>As stated before, we&#x27;ll use gitlab to store our ansible files and use it as a source for the AWX project. AWX will query the gitlab repo and update its files (playbooks, inventories, etc.) on demand (or automatically if configured in this way). Also, please note that since this data (playbooks, inventories) have different objectives and requirements from our structured data created in the previous section you should use a separate gitlab or at least use specific permissions to allow only AWX to read this repo (not modify!) and only enable write permissions on an on-demand basis to admins/operators (eg.: update a playbook).</p><p>If you have a big automation team or several teams with different roles on IT automation you could have different gitlab repositories with different playbooks, inventories, templates, etc. and each of them only reading/modifying files on their repos. This way you minimize the permission requirements and divide risks.</p><p>You can create the repo and submit all files on gitlab using the web interface or git, or you can use the script on the full-stack automation repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>) and execute the main AWX demo script (sudo /bin/bash ./demo/awx/import_files_on_gitlab.sh)</p><h2>Configuring AWX to use inventory, playbook and template</h2><p>Now, since ansible files are on gitlab we need to configure AWX to integrate with gitlab to get the files and also create our inventory and job templates that will run our playbook (<a href="https://docs.ansible.com/ansible-tower/latest/html/userguide/job_templates.html">AWX job template</a> is where we link AWX to our playbook). We will also create a workflow to run our playbook because of a Ansible &quot;limitation&quot; on how to manipulate variables on &#x27;import_playbook&#x27; directive.</p><p>You can do this using AWX web UI, but the <a href="https://github.com/liviozanol/full-stack_automation/blob/master/demo/awx/configure_awx.sh">script</a> will use AWX API, and will make the following tasks:</p><ul><li>Create a SCM credential (a gitlab deploy token) to allow AWX to login to gitlab.</li><li>Create an AWX project using our gitlab repository.</li><li>Create an inventory using our gitlab repo as a source.</li><li>Create one job template for the playbook that simply gets the network os name from inventory and stores on a variable accessible by other tasks on a workflow.</li><li>Create one job template for the playbook that implements the changes on our router (the main one that &#x27;imports&#x27; the real playbook).</li><li>Create a job workflow to link our 2 tasks and link the 2 tasks (these are 4 APIs calls).</li></ul><p>AWX final structure shown on its web UI
<img src="./img/awx.gif" alt="AWX final structure shown on its web UI"/></p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 6 - Demo -Scenario]]></title>
        <id>full-stack-it-automation-part-6-demo-scenario</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-6-demo-scenario"/>
        <updated>2021-12-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Demo Introduction]]></summary>
        <content type="html"><![CDATA[<h2>Demo Introduction {#demo-introduction}</h2><p>The past 6 posts were about building the infrastructure needed to implement the full-stack automation architecture described.</p><p>From now on, we will start to build a demo scenario to use it. As stated on the first post, we will use a scenario as an example and will use the architecture to automate one service.</p><h4>Assertions</h4><p>Consider these information assertions about you:</p><p>You work on a network service provider that serves WAN connections to several clients.</p><p>The network topology has CPEs (routers installed on clients premises) that is connected to the provider transport network. You are required to allow clients to modify some configurations on these CPEs.</p><ul><li><p>Clients should be able to change configuration for all CPEs installed on their sites.</p></li><li><p>Clients must only modify configuration for the LAN interface of CPEs.</p></li><li><p>Clients should be able to modify configuration for interface description (?), interface IP address/subnet, interface ACLs and helper/DHCP relay address.</p></li><li><p>Clients should be able to access a web page to make modifications. They should be able to also do these actions through an API.</p></li></ul><p>Now we can begin to define our service. Let&#x27;s call it &quot;Wan Sites&quot;. You could do something similar with other services, like &quot;DNS records&quot;, &quot;VMs&quot;, &quot;Containers&quot;, etc.</p><h2>Defining Structured Data {#structured-data}</h2><p>Considering the scenario described we can define a data structure that will be manipulated by users. Consider that users may view/change: ip address, description, helper address and ACLs on remote WAN sites.</p><p>We can create a structure comprehending these fields. In JSON it can be something like:</p><pre><code class="language-JSON">{
  &quot;site_id&quot;: &quot;(string)&quot;,
  &quot;custom_site_name&quot;: &quot;(string)&quot;,
  &quot;lan_interfaces&quot;: [
    {
      &quot;interface_name&quot;: &quot;(string)&quot;,
      &quot;ip_address&quot;: &quot;(ipv4/mask)&quot;,
      &quot;description&quot;: &quot;(string)&quot;,
      &quot;helper_address&quot;: &quot;(ipv4)&quot;,
      &quot;in_acl&quot;: [
        {
          &quot;action&quot;: &quot;(permit|deny)&quot;,
          &quot;src&quot;: &quot;(ipv4/mask)&quot;,
          &quot;dst&quot;: &quot;(ipv4/mask)&quot;,
          &quot;protocol&quot;: &quot;(tcp|udp|icmp|ip|gre)&quot;,
          &quot;port&quot;: &quot;(1-65535)&quot;
        }
      ],
      &quot;out_acl&quot;: [
        {
          &quot;action&quot;: &quot;(permit|deny)&quot;,
          &quot;src&quot;: &quot;(ipv4/mask)&quot;,
          &quot;dst&quot;: &quot;(ipv4/mask)&quot;,
          &quot;protocol&quot;: &quot;(tcp|udp|icmp|ip|gre)&quot;,
          &quot;port&quot;: &quot;(1-65535)&quot;
        }
      ]
    }
  ]
}
</code></pre><p>We could also start to think on more specific constraints and business rules that our service (and structured data) will have. For example:</p><ul><li>The network only supports IPv4 (for simplicity on this demo).</li><li>custom<em>site_name is a field used by the client to name its site for its own way of organizing. Must only have letters, numbers, spaces and &quot;-&quot; or &quot;</em>&quot;. Should have a maximum of 20 characters.</li><li>interface_name is a string identifying the interface on the equipment (e.g.: gigabitethernet 0) and must not be edited by users.</li><li>description must only have letters, numbers, spaces and &quot;-&quot; or &quot;_&quot;. Should have a maximum of 20 characters.</li><li>ip_address, acl &quot;src&quot; and &quot;dst&quot;, must be expressed as ipv4 &quot;/&quot; mask bits (e.g.: 192.168.0.1/32).</li><li>helper_address must have only one ipv4 address (for simplicity in this demo).</li></ul><p>ACL specific constraints:</p><ul><li>action must be either &quot;permit&quot; or &quot;deny&quot;.</li><li>protocol must be one of the following: tcp, udp, icmp or gre.</li><li>icmp type and code are not supported.</li><li>For protocol types ip, icmp and gre, &quot;port&quot; must be empty.</li><li>For protocol types tcp and udp, ports may be empty (any port), a number (e.g.: 80) or a range separated by a dash (e.g: 22-636).</li><li>Each ACL should have a maximum of 20 ACL rules/lines (aka ACEs).</li><li>src or dst IPs MUST be on interace LAN range. (e.g.: LAN=192.168.0.1/24. dst or src IP MUST be inside this range. (e.g.: 192.168.0.5/32 or 192.168.0.0/24) )</li></ul><h2>Gitlab Structure</h2><p>Since gitlab is being used as the place to store the data for all automated services, we must define how to use its structure to make it easy (and usable) to manipulate data for each of these. Gitlab has a feature called <a href="https://docs.gitlab.com/ee/user/group/subgroups/">groups and subgroups</a> that allows us to create a hierarchical tree for our projects.</p><p>Projects are the places that effectively store versioned files. In this case our structured data will be stored on these projects.</p><p>We will use the following hierarchy:</p><ul><li>For every automated service we will create a group.</li><li>For each client/tenant for each service, we will create a subgroup.</li><li>Inside every subgroup we will have projects for each service instance (e.g.: Each CPE Wan Site).</li></ul><p>Considering the above, we will have a group called &quot;wan_sites&quot; for our service. Any other service will have a new group (e.g.: &quot;vms&quot;, &quot;containers&quot;, etc.).
We will also have 2 subgroups: &quot;client_a&quot; and &quot;client_b&quot; (could be any number of subgroups. a.k.a. &#x27;clients/tenants&#x27;).
For each subgroup we will create 2 projects: site_1, site_2 (client A); site_3, site_4 (client B).
On each project we will store a file named &quot;wan_site_cfg.json&quot; that will store our structured JSON data detailed on the previous topic above.</p><p>So, if you want to get data from site_2, you can access the path &quot;https://gilab_url/wan_sites/client_a/site_2/wan_site_cfg.json&quot;.</p><p>Except for project name (only in our demo), these names and paths aren&#x27;t visible to the final user, so you can make your own structure and name it the way you want just adjusting your API. Example: client-&gt;service-&gt;service_instance; service-&gt;service_instance (client is a field on each instance).</p><p>Using gitlab to store this structured data have a drawback on speed and functionalities, since we are using files to store data (even that they are stored on gitlab), groups/&quot;folders&quot; to separate clients/services and need to make more HTTP requests to get our information. If you need more speed or a more structured way of doing this, you can use something like a database or <a href="https://netbox.readthedocs.io/en/stable/">Netbox</a> or Python Eve like stated before. <a href="https://docs.python-eve.org/en/stable/">Python Eve</a> is REALLY good for this, and you can identify resources per client on the data itself without relying on folders/groups (e.g.: a simple field called &quot;client&quot; on the example JSON on the previous section). It also has built-in sort and filter functions using mongodb style query.</p><p>Final gitlab structure as shown on web UI
<img src="./img/gitlab.gif" alt="Final gitlab structure as shown on web"/></p><h2>Creating Gitlab Structure</h2><p>You can create the structure using the web interface, creating the wan_sites group, clients subgroups, sites projects and manually creating the a dummy wan_site_cfg.json file on each, or you can use the script on the full-stack automation repo (<a href="https://github.com/liviozanol/full-stack-automation">https://github.com/liviozanol/full-stack-automation</a>) and executing the main demo script (sudo /bin/bash ./demo/gitlab/config_gitlab.sh)</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 5 - Gitlab CI]]></title>
        <id>full-stack-it-automation-part-5-gitlabci</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-5-gitlabci"/>
        <updated>2021-12-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Gitlab-CI]]></summary>
        <content type="html"><![CDATA[<h2>Gitlab-CI {#gitlab-ci}</h2><p>Gitlab-CI, AKA Gitlab runner, is a Continuous Integration and Continuous Delivery/Deployment solution built by gitlab. It fully integrates with gitlab listening for changes on repositories and executing pipelines to automate build, test, approve and deploy applications.</p><p>It can run on a container and, after a change is triggered, uses docker images to execute steps configured on the pipeline.</p><p>More about Gitlab can be found on its site: <a href="https://docs.gitlab.com/ee/ci/">https://docs.gitlab.com/ee/ci/</a></p><h2>Gitlab-CI Role {#gitlab-ci-role}</h2><p>Gitlab-CI will be used as the mechanism that starts automation. It will monitor gitlab repositories, detect changes on it and call AWX to implement changes on automated resources (eg.: router). This is its main function, but it will also be used to automatically create containers needed by the API and web interface.</p><p>Gitlab-CI talks to almost every component on the solution:</p><ul><li>Calls AWX API to run and monitor jobs/playbook execution;</li><li>Monitor changes on gitlab repositories to run its CI/CD pipelines;</li><li>Create API containers and also talk to them if needed on a pipeline step (should be a separated gitlab runner);</li><li>Get keys/secrets from hashicorp vault 2 if needed on the pipeline and use them to talk to gitlab itself or to talk to full-stack automation APIs.</li></ul><p>Gitlab-CI will be running with privileged mode so it can access docker socket and create, update and delete containers on the host itself. This is needed because Gitlab-CI will be responsible for managing API and web interface containers.</p><p>For its automation function on this architecture itself gitlab-ci only needs to get data on gitlab and call AWX REST API, so no privileged mode is required and you could use something like <a href="https://hub.docker.com/_/docker">docker in docker</a> to run it or a direct shell script. It is a good idea to separate functions as stated before.</p><p><img src="./img/gitlab-ci_arch.svg" alt="Gitlab-CI role on architecture"/></p><h2>Gitlab-CI Alternatives {#gitlab-ci-alternatives}</h2><p>You can change Gitlab-CI to other CI/CD tools very easily. It is important to note that we are using it because of its tighty integration with gitlab itself and we will be able to query the gitlab API to know which step we are in the pipeline so we can notify the user about its request.</p><p>So, feel free to use other tools like CircleCI, Jenkins, Travis, whatever.</p><p>You could also choose to not use a CI/CD as an element that starts automation. If you decide, for example, to use a solution with python EVE as a database and a MQ to talk to AWX you could have the jobs and approval steps controlled by the API directly with its status stored on python EVE and MQ could be the responsible to query AWX job status and update it on EVE.</p><h2>Gitlab-CI Installation and Configuration {#gitlab-ce-install-config}</h2><p>First, if you haven&#x27;t cloned the architecture repo from github, please do so: <code>git clone https://github.com/liviozanol/full-stack-automation</code></p><p>TL;DR: Simply run the shell script</p><pre><code class="language-shell">sudo /bin/bash installation/gitlab-runner/create_gitlabrunner.sh
</code></pre><p>:::note
You need to have docker available. Docker service must be running and healthy (check with <code>sudo docker ps</code> or similar).
:::</p><p>Installing gitlab-runner is simple as issuing a docker run</p><pre><code>sudo docker run -d --name gitlab-runner --restart always \
    --privileged \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /srv/gitlab-runner:/etc/gitlab-runner:Z \
    gitlab/gitlab-runner:alpine
</code></pre><p>In the next steps, gitlab-runner will be configured to register on gitlab and monitor changes.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 4 - AWX]]></title>
        <id>full-stack-it-automation-part-4-awx</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-4-awx"/>
        <updated>2021-12-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Ansible/AWX]]></summary>
        <content type="html"><![CDATA[<h2>Ansible/AWX {#ansible-awx}</h2><p>Ansible is an automation platform that supports a wide range of functions and can be used to automate almost anything: From network devices from different manufacturers to custom web APIs. The choice for ansible in this layer is for this reason and also because of AWX.</p><p>AWX is a complete automation solution based on ansible. It boosts ansible adding a lot of functionalities to make a complete automation tool mainly introducing a nice web system. Some cool functionalities includes:</p><ul><li>A web interface where you can make all life-cycle related activities with playbooks (monitor, execute, cancel, group, etc.). You still create the playbooks external.</li><li>Workflow of playbooks where you can execute one or more playbooks in sequence giving certain circumstances and even insert approval tasks!</li><li>Nice web forms with basic data validation to start playbooks execution.</li><li>Notification of jobs status using e-mail, telegram, rocketchat, etc.</li><li>Integration with git or other sources to read inventories and playbooks.</li><li>LDAP and other integrations to authentication/authorization.</li><li>And more important: A complete REST API that gives access to all of the above and more!</li></ul><p>Note that AWX comes with a form to play tasks/playbooks that could be used for more simple automations. If you have small tight teams to OAMP only some infrastructure you could use it directly. But secrets for automated resources could be leaked and I do not think that this is a good solution for end-user direct intervention. So do it at your own risk.</p><p>:::note
To execute any playbook you can (and <em>really</em> should) add a bastion host to access your automated elements, and make AWX use, for example, SSH tunnel prior to connecting to the automated element, protecting it even more from undesirable access. <a href="https://docs.ansible.com/ansible/5/reference_appendices/faq.html#how-do-i-configure-a-jump-host-to-access-servers-that-i-have-no-direct-access-to">Read more on ansible documentation.</a>.
:::</p><h2>Ansible/AWX Role {#ansible-role}</h2><p>Ansible/AWX is the &quot;engine&quot; that will access automation elements and execute actions to implement the changes submitted by the user (i.e. run a playbook) and validated by API and CI/CD.</p><ul><li>It talks to any automated resource to execute change actions.</li><li>It talks to hashicorp vault 1 to get keys/pass to access the automated resources</li><li>It talks to gitlab to get inventories, playbooks and complementary files (templates, etc.).</li><li>It receives API requests from Gitlab CI to start and monitor jobs/playbooks.</li></ul><p>Even if AWX has an approval task on workflows, the solution described here won&#x27;t use it. Since AWX has access to sensitive data, like key or pass to access infrastructure resources, we need to minimize any human intervention on it and also its point of contact with other infrastructure. Only gitlab CI/CD will talk actively to AWX on full-stack automation architecture. AWX solution will be used solely as a way to start and monitor tasks execution.</p><p><img src="./img/awx_arch.svg" alt="AWX/ansible role on architecture"/></p><h2>Ansible/AWX Alternatives {#ansible-alternatives}</h2><p>You can use other automation tools you like, but since the solution is using AWX as an API to execute and monitor jobs, it&#x27;s important that you execute it using ansible playbooks. Since ansible can execute any arbitrary shell and python command (and others if you install it), you can use <a href="https://www.terraform.io/">terraform</a>, <a href="https://nornir.readthedocs.io/en/latest/">nornir</a>, <a href="https://napalm.readthedocs.io/en/latest/">napalm</a>, <a href="https://en.wikipedia.org/wiki/Expect">expect</a>, custom scripts, whatever, but it needs to be called from ansible. You can, for example, install custom linux packages on ansible job execution containers and just call a shell script in a playbook.</p><p>Yes, ansible can be <a href="https://networklore.com/ansible-nornir-speed/">slow in some situations</a>, and templates or <a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_templating.html">jinja2</a> like language sometimes are too much complex and boring to write, but for this architecture, objetiving a manufacturer agnostic and wide range automation, its important. If you need high speeds or don&#x27;t like ansible at all, you can use your preferred automation tool through ansible as explained, or just build a new API for you custom automation tool - if it doesn&#x27;t have (and insert a MQ solution as stated on gitlab session).</p><h2>Ansible/AWX Installation and Configuration {#ansible-install-config}</h2><p>First, if you haven&#x27;t cloned the architecture repo from github, please do so: <code>git clone https://github.com/liviozanol/full-stack_automation</code></p><p>TL;DR: Simply run the shell script</p><pre><code>sudo /bin/bash installation/awx/create_awx.sh
</code></pre><p>In this guide AWX/ansible will be installed using docker-compose (which is not indicated for production - should be kubernetes instead). Will be used the <a href="https://github.com/ansible/awx/blob/devel/tools/docker-compose/README.md">&quot;official&quot; awx docker-compose guide</a>.</p><p>:::note
You need to have jq (used by script), ansible, openssl, docker and docker-compose available. Docker service must be running and healthy (check with <code>sudo docker ps</code> or similar).</p><p>As of 2021-12-07, you also need to be able to execute <code>make</code> (i.e.: build-essential, make, automake, gcc, etc.).</p><p>The steps below are already done by the script, which also makes some bugfixes. They are here only for information.
:::</p><p>1- Clone AWX Repo (version 19.5.0)</p><pre><code class="language-shell">git clone -b 19.5.0 https://github.com/ansible/awx.git
</code></pre><p>2- CD to the directory</p><pre><code class="language-shell">cd awx
</code></pre><p>3- Change passwords/secrets</p><pre><code class="language-shell">sed -i &#x27;s/# pg_password=&quot;&quot;/pg_password=&quot;fullstack_automation_pg&quot;/g;s/# broadcast_websocket_secret=&quot;&quot;/broadcast_websocket_secret=&quot;fullstack_automation_broadcast_websocket&quot;/g;s/# secret_key=&quot;&quot;/secret_key=&quot;fullstack_automation_secret&quot;/g&#x27; tools/docker-compose/inventory
</code></pre><p>4- Build the image</p><pre><code class="language-shell">make docker-compose-build
</code></pre><p>5- Run AWX</p><pre><code class="language-shell">make docker-compose COMPOSE_UP_OPTS=-d
</code></pre><p>6- Enable WEB UI (not required but good for troubleshooting)
:::warning
Please wait about 5 minutes so AWX is up and running before issuing this command
:::</p><pre><code class="language-shell">docker exec tools_awx_1 make clean-ui ui-devel
</code></pre><p>7- Change default admin password</p><pre><code class="language-shell">docker exec -it tools_awx_1 bash -c &quot;awx-manage update_password --username=admin --password=awx-fullstackautomationpass&quot;
</code></pre><hr/><p>Below steps can also be done using AWX web UI, but for simplicity, everything will be done using its REST API.</p><p>8- Create an organization that will be used to further commands</p><pre><code class="language-shell">AWX_ORG_ID=`curl -sk --request POST https://admin:awx-fullstackautomationpass@localhost:8043/api/v2/organizations/ -H &quot;Content-Type: application/json&quot; --data &#x27;{&quot;description&quot;: &quot;full stack organization&quot;, &quot;name&quot;: &quot;FULLSTACK_INC&quot;}&#x27; | jq .id`

echo &quot;Adding Default Galaxy Credential to Organization. (Needed to install custom collections from ansible galaxy)&quot;
curl -sk --request POST https://admin:awx-fullstackautomationpass@localhost:8043/api/v2/organizations/$AWX_ORG_ID/galaxy_credentials/ -H &quot;Content-Type: application/json&quot; --data &#x27;{&quot;id&quot;: 2}&#x27; | jq .id
</code></pre><p>9- Create a user that will be used by GITLAB-CI to start and monitor jobs/playbooks</p><pre><code class="language-shell">AWX_USER_ID=`curl -sk --request POST https://admin:awx-fullstackautomationpass@localhost:8043/api/v2/organizations/$AWX_ORG_ID/users/ -H &quot;Content-Type: application/json&quot; --data &#x27;{&quot;email&quot;: &quot;user@fullstackapi.io&quot;, &quot;first_name&quot;: &quot;fullstack&quot;, &quot;is_superuser&quot;: false, &quot;last_name&quot;: &quot;api&quot;, &quot;password&quot;: &quot;fullstackapi_pass&quot;, &quot;username&quot;: &quot;fullstackapi&quot; }&#x27; | jq .id`

</code></pre><p>10- Create a credential type to store harshicorp vault token so it can be used later to request secrets. You could try to use the specific harshicorp vault credential type that AWX already has, but we&#x27;ll be using a custom one</p><pre><code class="language-shell">AWX_VAULT_CREDENTIAL_TYPE=`curl -sk --request POST https://admin:awx-fullstackautomationpass@localhost:8043/api/v2/credential_types/ -H &quot;Content-Type: application/json&quot; --data &#x27;{&quot;name&quot;: &quot;Custom Vault Cred Type&quot;,&quot;description&quot;: &quot;Custom credential type to store vault token&quot;,&quot;kind&quot;: &quot;cloud&quot;,&quot;inputs&quot;: { &quot;fields&quot;: [ {&quot;id&quot;: &quot;vault_server&quot;,&quot;type&quot;: &quot;string&quot;,&quot;label&quot;: &quot;URL to Vault Server (i.e: http://127.0.0.1:5555/)&quot; },{ &quot;id&quot;: &quot;vault_token&quot;,&quot;type&quot;: &quot;string&quot;,&quot;label&quot;: &quot;Vault token&quot;,&quot;secret&quot;: true}],&quot;required&quot;: [&quot;vault_server&quot;,&quot;vault_token&quot;]},&quot;injectors&quot;: { &quot;extra_vars&quot;: { &quot;vault_server&quot;: &quot;{{ vault_server }}&quot;,   &quot;vault_token&quot;: &quot;{{ vault_token }}&quot;}}}&#x27; | jq .id`
</code></pre><p>11- Create a credential to store vault token and url, using the credential type just created</p><pre><code class="language-shell">AWX_VAULT_CREDENTIAL=`curl -sk --request POST https://admin:awx-fullstackautomationpass@localhost:8043/api/v2/organizations/$AWX_ORG_ID/credentials/ -H &quot;Content-Type: application/json&quot; --data &quot;{\&quot;credential_type\&quot;: $AWX_VAULT_CREDENTIAL_TYPE,\&quot;description\&quot;: \&quot;vault credential\&quot;, \&quot;inputs\&quot;:{\&quot;vault_server\&quot;:\&quot;http://127.0.0.1:9200\&quot;,\&quot;vault_token\&quot;:\&quot;fullstackautomation-root-token-vault2\&quot;}, \&quot;name\&quot;: \&quot;vault_credential\&quot;}&quot; | jq .id`
</code></pre><p>Everything is set to use AWX in the architecture. On further steps, after we create our repos on gitlab, we&#x27;ll be creating projects, inventories and templates. All linked to the gitlab source.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 3 - Vault]]></title>
        <id>full-stack-it-automation-part-3-Vault</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-3-Vault"/>
        <updated>2021-12-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Harshicorp Vault]]></summary>
        <content type="html"><![CDATA[<h2>Harshicorp Vault {#harshicorp-vault}</h2><p><a href="https://www.vaultproject.io/">Harshicorp Vault</a> is a solution to secure store and get sensitive information. It has a lot of features, like One Time Password, many auth methods, policies, temp tokens, etc.</p><p>For this solution will be used a basic auth with token (no refreshment) to query vault, simple Key/Value will store keys/passwords. Of course it can and should be improved, like using policies, temp tokens, other auth methods, lease duration, etc. You can (and should) also limit requests for secrets based on IP addresses.</p><h2>Harshicorp Vault {#vault-role}</h2><p>The full-stack automation solution will have 2 vaults.</p><p><img src="./img/vault_arch.svg" alt="Vault role on architecture"/></p><p>Vault 1 only stores secret keys/values that will be used by AWX which are secrets that have access to modify infrastructure resources (e.g. router user/pass). A much more sensitive information.</p><p>Vault 2 stores secret keys/values to be used by the APIs, gitlab CI and AWX. It stores AWX user/pass and gitlab token. APIs need access to read/write on repositories of gitlab to change our structured data and will use the token stored on Vault for this. Gitlab-CI/Runner needs to start and read jobs on AWX using its API and will use user/pass stored on Vault for this.</p><p>Considering the sensitive data, vault 1 MUST only be accessed by AWX, and should preferably be installed in a specific network/security segment with ACLs/firewall rules permitting only AWX communication. This vault should only store credentials that will be used by the automation engine (ansible/awx).</p><p>Also, rules can be used inside Vault that permits reading key/pass only by AWX (IP based). In this way, even if the access token from AWX is leaked and even if for some reason an attacker can communicate with Vault, it won&#x27;t be able to get the sensitive key/pass.</p><p>On the &quot;demo&quot; solution, for simplicity, the 2 vaults will be on the same host, but in a production environment must be separated. Also, will be used the &quot;dev&quot; mode with local file storage and no TLS, but in a production environment you must, of course, install it with TLS.</p><p>Also, on demo, Vault 2 will store user credentials (user/pass) that are used by end users to query API and also used on the UI for authentication. On production, you should consider using other authentication/authorization method for them.</p><h2>Harshicorp Vault Alternatives {#vault-alternatives}</h2><p>You can use other methods or solutions if wanted like: gitlab secret variables; ansible/awx own vault; azure/aws vault; whatever. Harshicorp vault is used because it&#x27;s a great software, with nice API, has enhanced security and is cloud/architecture agnostic. Also it has only one separate function in the solution, so, if needed, it can be easily changeable.</p><h2>Harshicorp Vault Installation and Configuration {#vault-install-config}</h2><p>First, if you haven&#x27;t cloned the architecture repo from github, please do so: <code>git clone https://github.com/liviozanol/full-stack-automation</code></p><p>TL;DR: Simply run the shell script</p><pre><code>sudo /bin/bash installation/vault/create_vaults.sh
</code></pre><p>In this guide Harshicorp Vault will be installed using docker-compose.</p><p>:::note
You need to have docker and docker-compose available. Docker service must be running and healthy (check with <code>sudo docker ps</code> or similar).
:::</p><p>The installation has a docker-compose YAML and a shell script. The shell script is used to create the key/pass on the 2 vaults that will be used to authenticate on AWX, gitlab and on the automated element. Note that vault is running only on HTTP. In a production environment you should run it on HTTPS (offloaded or not).</p><p>The docker-compose will install and run vault.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 2 - Gitlab]]></title>
        <id>full-stack-it-automation-part-2-gitlab</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-2-gitlab"/>
        <updated>2021-12-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Gitlab]]></summary>
        <content type="html"><![CDATA[<h2>Gitlab {#gitlab}</h2><p>Since gitlab is the center of our architecture we&#x27;ll start by installing and configuring it.</p><p>Gitlab is a git source code controller with a nice websystem that doesn&#x27;t need introductions and is very similar to github. It has versioning, issues control, tags, auto devops, and a lot of features that you can explore.</p><p>More about Gitlab can be found on its site: <a href="https://about.gitlab.com/stages-devops-lifecycle">https://about.gitlab.com/stages-devops-lifecycle</a></p><h2>Gitlab Role {#gitlab-role}</h2><p><img src="./img/gitlab_arch.svg" alt="Gitlab role on architecture"/></p><p>Gitlab will be used by the solution for 3 main objectives:</p><ul><li><strong>Store structured data that will be read and manipulated by the API, also keeping historical data/versions.</strong> This is it purpose on architecture.</li><li>Store inventories, playbooks and complementary files (templates, etc.) used by AWX. (Could be files directed inserted on AWX)</li><li>Store the APIs and UI code. (Could be files directed created on a web server infrastructure)</li></ul><p>Ideally, for security purposes, you should separate it in 3 or 2 gitlabs (1 only for AWX files, <strong>1 for your structured data</strong> (our main reason) and 1 for APIs/UI). Gitlab-ci/runner should communicate only with structured data gitlab and maybe APIs/UI gitlab. For simplicity on the demo, will be kept only one.</p><p>On full-stack automation, gitlab never talks to any external element and only receives connection from the following elements:</p><ul><li>APIs that need to read/write structured data on repositories.</li><li>Gitlab CI that reads pour structured data and starts the execution of a pipeline and call AWX to implement changes.</li><li>AWX that needs to read playbooks/inventories/templates (could be a separate gitlab).</li></ul><p>You may also need to manually send data to it (eg.: modify API source codes or ansible playbooks). Also it could be used to make a request/approval step for gitlab runner pipeline and &#x27;admins/operators&#x27; can access it and approve this step directly on gitlab web UI.</p><h2>Gitlab Alternatives {#gitlab-alternatives}</h2><p>As any other piece of the solution you could replace gitlab with other tools, like github, files, databases or whatever. Also, for the structured data for your API, you could use other specific solutions that should work even better than gitlab.</p><p>You could use <a href="https://netbox.readthedocs.io/en/stable/">Netbox</a> which already has a lot of structured data and a nice API, but it&#x27;s a little difficult to expand (as of 2021-12). You could also use <a href="https://docs.python-eve.org/en/stable/">Python Eve</a> which is a data agnostic API interface to a mongodb with features like data validation and format, auto sort/filter, historical data and included race conditions control. Really nice solution and fits perfectly on this architecture. Go check it out.</p><p>Remember that changing gitlab with another solution will need adjustment on the CI/CD portion. You can add an MQ (eg.: activeMQ, rabbitMQ) to the architecture that would make you less dependent on gitlab CI and make gitlab more easily replaceable. MQ would be the mechanism that calls AWX and monitor execution (instead of gitlab-ci), and the API itself would post jobs on the MQ. This would also allow you to insert other automations tools besides AWX, since the MQ would be the solution that calls jobs and monitors its execution. It is a good option also.</p><h2>Gitlab Installation and Configuration {#gitlab-install-config}</h2><p>First, if you haven&#x27;t cloned the architecture repo from github, please do so: <code>git clone https://github.com/liviozanol/full-stack-automation</code></p><p>TL;DR: Simply run the shell script. You could check each step reading the <a href="https://github.com/liviozanol/full-stack_automation/tree/master/installation/gitlab">repo itself</a></p><pre><code>sudo /bin/bash installation/gitlab/create_gitlab.sh
</code></pre><p>:::note
You need to have docker and docker-compose available. Docker service must be running and healthy (check with <code>sudo docker ps</code> or similar).
:::</p><p>The installation has a docker-compose YAML and a shell script. The shell script is used to create users and access tokens on gitlab that will be used by AWX to get playbooks and also for further scripts to associate gitlab-ci to projects programmatically (and also create the repositories). Note that we are running gitlab only on HTTP. In a production environment you should run it on HTTPS (offloaded or not).</p><p>The docker-compose will install and run gitlab and also run a shell script that will create users access tokens to be used further on the solution.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Stack Automation Part 1 - Overview]]></title>
        <id>full-stack-it-automation-part-1</id>
        <link href="https://livio.zanol.com.br/full-stack-it-automation-part-1"/>
        <updated>2021-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2>Introduction {#introduction}</h2><p>So, this is the first post on a series of automation posts that will demonstrate how to build a Full-Stack automation. In fact it can be used to automate any service, but we&#x27;ll focus and use network as an example.</p><p>This first post will cover the idea of the solution, its architecture, components and other useful information. I&#x27;ll try my best to not use marketing or specific definitions like SD-WAN, SDN, SaaS, IaaS, etc.</p><p>First of all, let&#x27;s be clear about what is considered &quot;full-stack automation&quot; regarding this series of posts. Full-stack automation is referred as a solution/architecture to fully automate network (or any other services), beginning on the automation itself (eg.: playbooks that will run commands on a Automated Resource) all the way up to a structured data source using gitlab, an API and a WEB interface to be used by final users of the solution.</p><p>After building all this architecture, users should be able to access a page, input some information/value in it (eg.: routes, acls, interfaces IP address, etc.) and watch it being deployed by ansible on a Automated Resource (eg.: router)</p><p><img src="./img/full-auto.gif" alt="Full-stack Auto GIF"/></p><p><strong>If you are on a hurry and don&#x27;t want to read further, the installation scripts and demo can be found in this repository</strong>: <a href="https://github.com/liviozanol/full-stack_automation">https://github.com/liviozanol/full-stack_automation</a>.</p><h2>The Architecture {#the-architecture}</h2><p>The architecture presented in this series of posts, which is shown below, is intended to be agnostic of manufacturer and cover any service automation, in a structured, secure and scalable way. Each piece was thought of in a way that can be exchanged by another solution with little/no adjustment on other pieces. You could change gitlab for files, change hashicorp vault for ansible secrets, change the API programming language to any other that you want, change the web user interface for other you want.</p><p><img src="./img/architecture_dark.svg" alt="Architecture of Full-Stack Automation"/></p><p><strong>This series of posts will detail every layer/element of the architecture, also providing a guide on how to install and configure each one of them. In the end you should have a complete functional Full-Stack Automation solution based on this architecture and also a demo.</strong></p><p>Description of each element/layer:</p><ul><li><p>The web interface will provide a friendly interface to users where they can list, create, update or delete data from a browser.</p></li><li><p>The API will provide a single point of contact for all automation and is responsible to receive user requests on data modifications (eg.: change network interface ip address), validate the data and send it to gitlab.</p></li><li><p>Gitlab is used as a place to store structured data from users (like a database), and also ansible playbooks.</p></li><li><p>Gitlab-CI/Runner will listen for data modifications on gitlab and call AWX to execute playbooks.</p></li><li><p>AWX/ansible will execute playbooks to implement the data modified by users in Automated Resources.</p></li><li><p>Hashicorp Vault will store key/pass to access Automated Resources and also key/pass to interact between elements (git lab token, etc.).</p></li><li><p>Automated Resource is a piece of infrastructure that provides some IT service. It can be a router/switch, a firewall, a hypervisor (eg.: vmware vsphere, openstack), a virtual machine, a container, etc. A router will be used as an example. Since we are using ansible, almost anything can be automated. You can even manage a terraform deployment to extend it even further (<a href="https://docs.ansible.com/ansible/latest/collections/community/general/terraform_module.html">https://docs.ansible.com/ansible/latest/collections/community/general/terraform_module.html</a>)! The resource is <em>only accessed by ansible</em> and using credentials stored in a specific Hashicorp Vault. You can also use other middle/tunnel/bastion infrastructure to access the Automated Resource through ansible, like a SSH tunnel.</p></li></ul><h2>Why/When to use Full-Stack Automation? {#why-use-archtecture}</h2><p>When you have services/functions that needs to be used by final users to manage a provided infrastructure by you. Examples:</p><ul><li><p>You are a cloud provider and your users needs a portal/API/console/terraform module to manage the infrastructure you provide (just like current cloud providers: AWS, Azure, Google Cloud, Digital Ocean, etc.);</p></li><li><p>You are a service provider and you want(or need) that your users be able to change some configurations remotely for the provided infrastructured using a portal/API. (eg.: create ACLs on a remote CPE router you provide for them)</p></li><li><p>You have an operation team and you want to provide a portal/API to operators to provise or change infrastructure. (eg.: install and manage CPEs on a WAN network)</p></li><li><p>You just want to test the architecture or learn more about automation.</p></li></ul><h2>Why/When <em>NOT</em> use Full-Stack Automation? {#why-not-use-archtecture}</h2><p>When you don&#x27;t need/want to provide infrastructure services in a self-service manner to the end user or When you have a small team that can manage the infrastructure directly.</p><p>Either way, the architecture designed can also be used for any cases on any element. You can, for example, just provide access to gitlab files and let your technical team modify direct information (a.k.a gitops) and leverage on the CI/CD + AWX solutions to implement it on your infrastructure. You can also provide direct access to AWX so operators just complete a web form (like a normal AWX use).</p><h2>Automation Scenario Used {#automation-scenario}</h2><p>We will use a scenario as an example to implement a demo for the Full-Stack Automation architecture described here.</p><p>Assertions:</p><p>You work on a network service provider that serves WAN connections to several clients.</p><p>The network topology has CPEs (routers installed on clients premises) for every remote site that is connected to the provider network and that router connects each client site LAN to your transport network. You are required to allow clients to modify some configurations on these CPEs.</p><ul><li><p>Clients should be able to change configuration for all CPEs installed on their remote sites.</p></li><li><p>Clients must only modify configuration for the LAN interface of CPEs.</p></li><li><p>Clients should be able to modify configuration for interface description, interface IP address/subnet, interface ACLs and helper address/dhcp relay (DHCP server address).</p></li><li><p>Clients should be able to access a web page to make modifications. They should be able to also do these actions through an API.</p></li></ul><h2>System Requirements {#system-requirements}</h2><p>The simulated architecture will run on a single machine. In a production environment you may need to adjust it by separating elements in different secure segments and adding one or more pieces of softwares as needed.</p><p>Since it&#x27;s on a single machine you&#x27;ll need some good amount of <strong>FREE</strong> RAM memory (around 8GB), around 20GB of free disk space and a decent CPU. Since this is a demo, you have the option to use gitlab.com (or other gitlab) instead of intalling one and save about 3-4GB of RAM, be needs to adjust the steps/scripts.</p><p>It&#x27;ll be used docker-compose and some custom scripts to build the solution. You could install it using pure docker or kubernetes, adjusting scripts and files, of course.</p><p>You can run the scripts on a VM using a virtual box, another hypervisor (vmware, &#x27;aws&#x27;, openstack, etc) or use WSL on windows (which is also a VM by the way). Running the demo on a VM is a nice option.</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First Post]]></title>
        <id>first</id>
        <link href="https://livio.zanol.com.br/first"/>
        <updated>2021-11-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Just a first page to keep a record (a.k.a "Hello World")!]]></summary>
        <content type="html"><![CDATA[<p>Just a first page to keep a record (a.k.a &quot;Hello World&quot;)!</p>]]></content>
        <author>
            <name>Lívio Zanol Puppim</name>
            <uri>https://github.com/liviozanol</uri>
        </author>
    </entry>
</feed>